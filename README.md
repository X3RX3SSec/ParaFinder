# üï∑Ô∏è X3RX3S Parameter Finder

> A multithreaded, blazing-fast URL parameter discovery tool for recon ninjas.  
> Built for speed, stealth, and sexy terminal vibes.  
> Made by **X3RX3S** ‚Äî use responsibly üíÄ

---

## üöÄ What is this?

**X3RX3S Parameter Finder** is a CLI tool that crawls a website and identifies URLs containing common GET parameters. It uses threading for speed, optional user-agent rotation for stealth, supports proxies and cookies, and can output results in TXT, CSV, or JSON formats.

This is for **bug bounty hunters**, **pentesters**, or anyone doing **web app recon** who wants to quickly gather parameterized URLs for fuzzing, testing, or exploitation.

---

## üéØ Features

- ‚úÖ Multi-threaded crawling
- ‚úÖ Finds **40 (default)** or **100+ (supreme)** common parameters
- ‚úÖ Beautiful **colorful banner** with author credit
- ‚úÖ Terminal-friendly **light blue output**
- ‚úÖ Optional **User-Agent randomization**
- ‚úÖ **Proxy** support (`http`, `https`)
- ‚úÖ **Custom cookies** for authenticated crawling
- ‚úÖ **Crawl depth** control
- ‚úÖ Output in **TXT**, **CSV**, or **JSON**
- ‚úÖ **Progress bar** via `tqdm`
- ‚úÖ Clean, lean, and fast

---

## üì¶ Installation

Clone the repo and install dependencies:

```bash
git clone https://github.com/YOURUSERNAME/x3rx3s-paramfinder.git
cd x3rx3s-paramfinder
pip install -r requirements.txt
```

> You only need:
> - `requests`
> - `beautifulsoup4`
> - `tqdm`

---

## üõ†Ô∏è Usage

```bash
python parameter_finder.py <URL> [options]
```

### üîß Options

| Option                | Description                                                                 | Default       |
|----------------------|-----------------------------------------------------------------------------|---------------|
| `--mode`             | Parameter mode: `default` (40) or `supreme` (100+)                          | `default`     |
| `--threads`          | Number of threads to use for crawling                                       | `10`          |
| `--user-agent-random`| Randomizes the `User-Agent` header for each request                         | Off           |
| `--output-file`      | Save results to a file                                                      | None          |
| `--output-format`    | File format: `txt`, `json`, or `csv`                                        | `txt`         |
| `--proxy`            | Use a proxy (e.g., `http://127.0.0.1:8080`)                                 | None          |
| `--cookie`           | Send cookies (e.g., `"PHPSESSID=abc123;"`)                                  | None          |
| `--depth`            | How deep to crawl: `1` (just homepage links), `2` (links of links), etc     | `2`           |

---

## üß™ Examples

### Crawl with defaults:

```bash
python parameter_finder.py https://example.com
```

### Use supreme mode and random user-agent:

```bash
python parameter_finder.py https://example.com --mode supreme --user-agent-random
```

### Save to JSON file:

```bash
python parameter_finder.py https://example.com --output-file out.json --output-format json
```

### Use proxy, cookies, and crawl depth 3:

```bash
python parameter_finder.py https://target.com \
  --proxy http://127.0.0.1:8080 \
  --cookie "auth=token123" \
  --depth 3
```

---

## üì§ Output

Results are printed in **light blue** in your terminal and can be saved as:

- `results.txt`
- `results.csv`
- `results.json`

Each file will contain only the URLs where at least **one common parameter** was detected.

---

## ‚ö†Ô∏è Legal Notice

This tool is intended for **authorized testing, learning, and bug bounty** purposes only.  
Do **NOT** use it on domains you don't own or have explicit permission to test.

You are responsible for your actions. I take no liability for misuse.

---

## üß† Tips

- Pipe output into other tools like `ffuf`, `xargs`, or `wfuzz`
- Combine with your own wordlists by modifying the `default_params` and `supreme_params` arrays
- Try crawling authenticated areas using `--cookie`
- Use `--depth 1` for fast shallow scans, `--depth 3+` for deeper crawls

---

## ü§ò Author

**X3RX3S**

Drop a ‚≠ê if this helped your recon flow.  
PRs welcome if you wanna contribute!

---